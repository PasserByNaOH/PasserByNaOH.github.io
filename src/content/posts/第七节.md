---
title: 强化学习第七节
published: 2024-05-28
description: 强化学习相关概念
tags: [概念积累, 网络安全, 强化学习RL]
category: RL
draft: false
---
# 第七节——值函数近似

## 1.目标函数

- $\hat{v}(s,w)$是对s的真值$v_{\pi}(s)$的一个估计值，目标是让估计值尽可能的接近真值

- 要找到最优的一个w（就是函数的参数），使$\hat{v}(s,w)$尽可能的接近真值$v_{\pi}(s)$

- 所以该问题是一个策略评估问题

  - 第一步，定义目标函数
  - 第二步，优化这个目标函数

   

### 目标函数的定义

$$
J(w)=\mathbb{E}[(v_{\pi}(S)-\hat{v}(S,w))^2]
$$

- 找到最优的w来优化目标函数J(w)

- S是一个随机变量，S的概率分布用stationary distribution表示

  stationary distribution(steady-state distribution/limiting distutribution)

  - 该概率分布描述了long-run behavior

    - long-run behavior 就从某一个状态出发，采取行动与环境进行交互，并一直采取策略非常多次，之后达到一种平稳状态，然后就可知每一个状态agent出现的概率

  - 表示为$\{d_{\pi}(s)\}_{s\in S}$，$d_{\pi}(s)$表示agent在s出现的概率

  - 则在该条件下，目标函数可表示为
    $$
    J(w)=\mathbb{E}[(v_{\pi}(S)-\hat{v}(S,w))^2]=\sum_{s\in S}d_{\pi}(s)(v_{\pi}(S)-\hat{v}(S,w))^2
    $$

### 目标函数的优化算法

使用梯度下降
$$
w_{k+1} = w_k-\alpha_k \nabla_w J(w_k)
$$
求导得真实梯度为
$$
\nabla_wJ(w) = -2\mathbb{E}[(v_{\pi}(S)-\hat{v}(S,w))\nabla_w\hat{v}(S,w)]
$$
由于真实梯度需要计算期望，故采用随机梯度来近似（因为不知道实际的模型，只能用随机梯度等方法来近似求解）（整合了2αt为αt）
$$
w_{t+1}=w_{t}+\alpha_t(v_{\pi}(s_t)-\hat{v}(s_t,w_t))\nabla_w\hat{v}(s_t,w_t)
$$
对于$v_{\pi}(s_t)$的替代有两种方法

- MC learning：用$g_t$代替，它为一个episode的discounted return，作为估计值
- TD learning：用$r_{t+1}+\gamma \hat{v}(s_{t+1},w_t)$代替 

### 线性函数近似

此时，$\hat{v}(s,w)=\phi(s)w$，则有$\nabla_w\hat{v}(s,w)=\phi(s)$
$$
w_{t+1}=w_{t}+2\alpha_t[r_{t+1}+\gamma \phi^T(s_{t+1})w_t-\phi^T(s_{t})w_t]\phi(s_t)
$$
该算法被称为**TD-Linear**

缺点

- 难以选取合适的特征向量

优点

- 能够很好的分析其理论性质
  - 具有比较强的表征能力

（tabular方法相当于每个点都精确，linear增加参数（如2次变为3次）就会导致计算效率变慢，最终变为tabular方法，这种情况有点像过拟合）



## 2.Sarsa的值函数近似表示法

$$
w_{t+1}=w_t+\alpha_t[r_{t+1}+\gamma \hat{q}(s_{t+1},a_{t+1},w_t)-\hat{q}(s_t,a_t,w_t)]\nabla_w \hat{q}(s_t,a_t,w_t)
$$

![image-20240529105451082](https://raw.githubusercontent.com/PasserByNaOH/PicGo/main/blogPic/image-20240529105451082.png)

## 3.Q-learning的值函数近似表示法

$$
w_{t+1}=w_t+\alpha_t[r_{t+1}+\gamma \max_{a\in \mathcal{A}(s_{t+1})} \hat{q}(s_{t+1},a,w_t)-\hat{q}(s_t,a_t,w_t)]\nabla_w \hat{q}(s_t,a_t,w_t)
$$

 ![image-20240529114453514](https://raw.githubusercontent.com/PasserByNaOH/PicGo/main/blogPic/image-20240529114453514.png)

## 4.Deep Q-learning or Deep Q-network (DQN)

### DQN的损失函数（目标函数）

$$
J(w)=\mathbb{E}[(R+\gamma \max_{a\in \mathcal{A}(S')} \hat{q}(S',a,w)-\hat{q}(S,A,w))^2]
$$

其中（S，A，R，S'）为随机变量

### DQN损失函数的优化

使用梯度下降，并假设$y=R+\gamma \max_{a\in \mathcal{A}(S')} \hat{q}(S',a,w)$里w为常数，此时更加容易求解J(w)

**方法1：使用两个网络**

为了实现以上的梯度下降，引入两个网络（函数）

- 主网络（main network）,用$\hat{q}(s,a,w)$表示，一直更新
- 目标网络，用$\hat{q}(s,a,w_T)$表示，不是一直更新，隔一段时间将main network的w赋值而得来

​	
$$
\nabla_wJ=\mathbb{E}[(R+\gamma \max_{a\in \mathcal{A}(S')} \hat{q}(S',a,w_T)-\hat{q}(S,A,w))\nabla_w \hat{q}(S,A,w)]
$$
使用两个网络的原因

- 计算梯度比较复杂，先固定一个（目标网络）再计算另一个（主网络） 

**方法2：经验回放(Experience replay)**

- 什么是经验回放？

  - 有序收集数据样本，但是使用的时候不一定按其先后顺序

  - 将所有数据放入一个集合当中,即replay buffer$\mathcal{B}={(s,a,r,s')}$

  - 每次训练神经网络，在replay buffer中取一些样本来训练

  - 由于这些样本已经存在，故使用这些样本被称为经验回放，取每个样本的概率是相同的，即服从均匀分布（uniform distribution）

    **个人理解**：书中有很多棋谱，agent学习了这些不同棋谱中的技巧，agent在下棋的时候利用这些技巧而获取胜利。

  

- 为什么用它？

  - 打破数据相关性，避免陷入局部最优解
  - 重复利用经验
  - 提高样本效率

![image-20240530112638740](https://raw.githubusercontent.com/PasserByNaOH/PicGo/main/blogPic/image-20240530112638740.png)
