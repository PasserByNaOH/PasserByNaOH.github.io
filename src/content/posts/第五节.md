---
title: 强化学习第五节
published: 2024-05-17
description: 强化学习相关概念
tags: [概念积累, 网络安全, 强化学习RL]
category: RL
draft: false
---
# 第五节——随机近似理论

## 1.求平均数

$$
w_{k+1}=w_{k}-\frac{1}{k}(w_k-x_k)
$$

其中$w_{k+1},w_k$表示当前状态的平均值(k)，和前一个状态的平均值(k-1)，$x_k$表示第k个数

**该式子表示了当前平均数与下一个平均数之间的关系**

将$\frac{1}{k}$替换为$\alpha_k$就得到一个一般化的式子
$$
w_{k+1}=w_{k}-\alpha_k(w_k-x_k), \ \ \alpha_k > 0
$$
此时$w_k$依然趋近于原平均值，该式是一个特殊的RM算法

## 2.RM（Robbins-Monro） Algorithm

$$
w_{k+1}= w_k - a_k \widetilde{g}(w_k,\eta_k),k=1,2,3,...
$$

其中，

- $w_k$表示对平均值的第k次估计
- $\widetilde{g}(w_k,\eta_k)=g(w_k)+\eta_k$表示带有噪音的一个观测
- $a_k$>0

该算法并不知道$g(w)$到底是什么（即是一个黑盒），算法依赖于以下数据

- 输入的$\{w_k\}$序列
- 测量到的$\{\widetilde{g}(w_k,\eta_k)\}$的值序列

## 3.SGD（Stochastic gradient descent）随机梯度下降

### SGD解决的问题

$$
\min_w J(w) = \mathbb{E}[f(w,X)]
$$

- 优化参数w使目标函数J(w)达到最小
- X是随机变量，期望是对于X的

**求解以上问的方法**

- 方法1：梯度下降gradient descent（GD）
  $$
  w_{k+1} = w_{k} - \alpha_k \nabla_w \mathbb{E}[f(w_k,X)]=w_{k} - \alpha_k \mathbb{E}[\nabla_wf(w_k,X)]
  $$
  其中，$\nabla_w$表示后面跟着的函数的梯度，$\alpha_k$表示步长，控制梯度方向下降的快慢

  **问题**：难以求出梯度的期望

- 方法2：批量梯度下降Batch gradient descent（BGD）

  由数据得到，对随机变量X进行采样$x_i$
  $$
  \mathbb{E}[\nabla_wf(w_k,X)]\approx \frac{1}{n}\sum^n_{i=1}\nabla_wf(w_k,x_i) \\
  
  w_{k+1} = w_{k} - \alpha_k\frac{1}{n}\sum^n_{i=1}\nabla_wf(w_k,x_i)
  $$

- **方法3**：SGD
  $$
  w_{k+1} = w_{k} - \alpha_k\nabla_wf(w_k,x_i)
  $$
  其中，$x_k$为X的一个随机采样

  - 对比于GD，将真正的梯度$\mathbb{E}[\nabla_wf(w_k,X)]$换成随机梯度$\nabla_wf(w_k,x_i)$
  - 对比于BGD，令n=1，表明只采样1次

## 4.BGD、MBGD和SGD的比较

$$
w_{k+1} = w_{k} - \alpha_k\frac{1}{n}\sum^n_{i=1}\nabla_wf(w_k,x_i) （BGD） \\
w_{k+1} = w_{k} - \alpha_k\frac{1}{m}\sum^n_{j\in\mathcal{I}_k}\nabla_wf(w_k,x_i) （MGBD） \\
w_{k+1} = w_{k} - \alpha_k\nabla_wf(w_k,x_i)（SGD）
$$

- BGD用了所有的采样，MBGD用所有采样中的一小部分，SGD随机用采样中的一个数据
- 相比于SGD，MBGD的随机性小于SGD
- 相比于BGD，MBGD随机性要大一些，要更加灵活和高效
