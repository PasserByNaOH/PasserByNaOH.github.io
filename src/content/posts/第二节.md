---
title: 强化学习
published: 2024-05-13
description: 强化学习相关概念
tags: [概念积累, 网络安全, 强化学习DL]
category: DL
draft: false
---
# 第二节——贝尔曼公式

## 1.State Value

$G_t$为一个轨迹的discounted return，则State value就是$G_t$的期望值Expectation
$$
v_\pi(s) = \mathbb{E}[G_t|S_t = s]
\\
v表示当前的value
\\
\pi表示当前策略policy
$$
state value与return的区别

- state value是多个trajectory的return的平均值
- return是单个trajectory的结果

## 2.贝尔曼公式

t时刻的价值函数为
$$
v_\pi(s) = \mathbb{E}[G_t|S_t = s]
$$
返回值$G_t$等于
$$
\begin{aligned}
G_t &= R_{t+1}+\gamma R_{t+2} + \gamma^2 R_{t+3} + …… \\
	&= R_{t+1}+\gamma (R_{t+2} + \gamma R_{t+3} + ……) \\
	&= R_{t+1} + \gamma G_{t+1}
\end{aligned}
$$
则价值函数可分为
$$
\begin{aligned}
v_\pi(s) &= \mathbb{E}[G_t|S_t = s]	\\
		 &= \mathbb{E}[R_{t+1}+\gamma G_{t+1}|S_t=s] \\
		 &= \mathbb{E}[R_{t+1}|S_t=s]① +\gamma\mathbb{E}[G_{t+1}|S_t=s]②

\end{aligned}
$$
其中

- ①式表示当前奖励的均值，用全概率公式分解
  $$
  \begin{aligned}
  \mathbb{E}[G_t|S_t = s]	
  &= \sum_{a}\pi(a|s)\mathbb{E}[R_{t+1}|S_t=s,A_t=a] \\
  &= \sum_{a}\pi(a|s)\sum_{r}\pi(r|s,a)r \\
  \end{aligned}
  $$
  其中$\pi(a|s)$表示采取采用**策略**$\pi$动作a的概率，$\mathbb{E}[R_{t+1}|S_t=s,A_t=a]$由期望的定义分解为$\sum_{r}\pi(r|s,a)r$

- ②式表示未来奖励的均值
  $$
  \begin{aligned}
  \mathbb{E}[G_{t+1}|S_t=s] 
  &= \sum_{s'}\mathbb{E}[G_{t+1}|S_t=s,S_{t+1}=s']p(s'|s) \\
  & 由于马尔可夫性质，即后面状态与前面无关，则\\
  &= \sum_{s'}\mathbb{E}[G_{t+1}|S_{t+1}=s']p(s'|s) \\
  &= \sum_{s'}v_\pi(s')p(s'|s)	\\
  &= \sum_{s'}v_\pi(s')\sum_{a}p(s' | s,a)\pi(a|s) 
  \end{aligned}
  $$
  a为动作集

故贝尔曼公式为
$$
\begin{aligned}
v_\pi(s) 
&= \mathbb{E}[R_{t+1}|S_t=s] +\gamma\mathbb{E}[G_{t+1}|S_t=s], \\
&= 	\sum_{a}\pi(a|s) \left[ \sum_{r}p(r|s,a)r+\gamma\sum_{s'}p(s’|s,a)v_{\pi}(s') \right], \forall s\in S.

\end{aligned}
$$
描述了不同状态之间的关系，对于所有状态都成立，即一个状态对应一个式子

##  3.Action value

定义
$$
q_\pi(s,a) = \mathbb{E}[G_t|S_t=s, A_t=a]
$$
表示从当前状态s出发，进行行动a后所得到return的平均值，依赖于策略$\pi$

有贝尔曼公式可得
$$
\begin{aligned}
q_\pi(s,a) 
&= \mathbb{E}[G_t|S_t=s, A_t=a] \\
&= \sum_{r}p(r|s,a)r+\gamma\sum_{s'}p(s’|s,a)v_{\pi}(s') 
\end{aligned}
$$

## **state value 与 action value的区别**

- state value: 从一个状态出发得到的平均返回值
- action value:从一个状态出发，采取一种行动后，得到的平均返回值

## 4.贝尔曼最优公式

$$
\begin{aligned}
v_\pi(s) 
&= 	\max_{\pi} \sum_{a}\pi(a|s) \left[ \sum_{r}p(r|s,a)r+\gamma\sum_{s'}p(s’|s,a)v_{\pi}(s') \right] \\
&= \max_{\pi}\sum_{a}\pi(a|s)q(s,a)
, \forall s\in S. \\
&= \max_{\pi}(r_{\pi} + \gamma P_{\pi}v) （向量表示）


\end{aligned}
$$

其中$p(r|s,a), p(s'|s, a), r, \gamma$已知，求解$v(s)，v(s')，\pi(a|s)$

当有最优策略$\pi^*$，则
$$
v^*= r_{\pi^*} + \gamma P_{\pi}v^*
$$
**$v^*$为最优解且唯一，$\pi ^*$不一定唯一**

## 5.压缩映射定理

#### 不动点

若存在映射$f:X \to X, 且x \in X$, 有$f(x) = x$，则称$x$为不动点

#### 压缩映射（压缩函数）

若映射$f$满足$||f(x_1) - f(x_2)|| \le \gamma ||x_1 - x_2||, \gamma \in (0,1)$， 映射$f$就称为压缩映射

#### 压缩映射定理性质

若有$压缩映射f，且x = f(x)$，则有

- 存在性：必定有一个不动点$x^*$满足$f(x^*) = x^*$
- 唯一性：不动点唯一
- 可求性（有一种算法能求）：有序列$\{x_k\}$且$X_{k+1} = f(x_k)$, 当$k\to \infty$则$x_k \to x^* $，收敛越来越块

