---
title: 强化学习
published: 2024-05-14
description: 强化学习相关概念
tags: [概念积累, 网络安全, 强化学习DL]
category: DL
draft: false
---
# 第三节——值迭代与策略迭代

## 1.值迭代算法

#### 定义

从初始值$v_0$开始，
$$
v_{k+1} = f(v_k) = \max_{\pi}(r_{\pi}+\gamma P_{\pi}v_k),\ \ k=1,2,3
$$

就是压缩映射中求解贝尔曼最优公式的方法，该算法包含两部分，已知$v_k$

- 策略更新，选择最优策略(该步骤是贪心的)
  $$
  \pi_{k+1} = \arg \max_{\pi}(r_{\pi} + \gamma P_{\pi}v_k)
  $$
  
- 值更新
  $$
  v_{k+1} = r_{\pi_{k+1}}+\gamma P_{\pi_{k+1}}v_k
  $$

## 2.策略迭代算法

从初始策略$\pi_0$开始，分为两步

- 第一步：值评估policy evaluation
  $$
  v_{\pi_k} = r_{\pi_k} + \gamma P_{\pi_k}v_{\pi_k}
  $$
  
- 第二步：策略提升（换为更好的策略）policy improvement
  $$
  \pi_{k+1}=\arg \max_{\pi}(r_{\pi}+\gamma P_{\pi}v_{\pi_k})
  $$
  

$$

$$

## 3.截断策略迭代算法（Truncated policy iteration algorithm）

#### 值迭代算法和策略迭代算法之间的区别

- 策略迭代算法从一个策略计算得到一个新的策略需要无穷多步，而值迭代算法只要一步（令$v_{\pi_1} = v_0$）

#### 截断策略迭代算法的由来

- 由于策略迭代算法从一个策略计算得到一个新的策略需要无穷多步，故取中间第j步（即策略迭代了j次，即截断）
- 当$j = 1$时，截断策略迭代算法就转换为了值迭代算法，当$j = \infty$时，就变为了策略迭代算法



