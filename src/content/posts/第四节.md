---
title: 强化学习第四节
published: 2024-05-15
description: 强化学习相关概念
tags: [概念积累, 网络安全, 强化学习DL]
category: DL
draft: false
---
# 第四节——蒙特卡洛方法

## 1.蒙特卡洛基本算法（MC Basic Algorithm）

蒙特卡洛基本算法是依赖于数据的，从一个初始的策略$\pi_0$出发，每次迭代都做以下两件事

- **策略评估policy evaluation**

  这一个步骤主要从所有$(s,a)$即所有状态和行动计算出$q_{\pi_k}(s,a)$

  即从$(s,a)$出发，经过所有行动到达目标而得到所有episode，再对episode的返回值求平均值

- **策略提升policy improvement**

  解决
  $$
  \pi_{k+1} = \arg\max_{\pi}\sum_{a}\pi(a|s)q_{\pi_k}(s,a), \ \ s\in S
  $$
  与策略迭代算法的第二步是一样的
  
  迭代次数，也是**episode length是足够有限长的**

与策略迭代算法的区别是

**策略迭代算法第一步求解state value，再得到action value**

**MC Basic 算法是直接通过数据得到$q_{\pi_k}$**

## 2.MC Exploring Starts——MC Basic算法的一个推广

#### Visit

在一个寻找过程epsidoe中，每一次出现的state-action pair，状态-行动对$(s_i,a_j)$，都称作一次**访问visit**

- 对于MC Basic算法，只使用初始的状态-行动对，用episode中剩下的实际return返回值来估计初始状态的action value行动值


所以出现了两种数据更新策略

- **first-visit method**：当一个状态-行动对第一次出现时，用后面的return来估计，而后面出现相同的状态-行动对就不再估计了。（一个episode可能会来回经过同一个状态，且在该状态做相同动作）
- **every-visit method**：不论状态-行动对是否曾出现，都可以用它后面的 return 估计初始的的 action value

#### GPI

Generalized policy iteration不是一个具体的算法，而是一大类算法。

- 其在策略评估和策略更新中不断的切换，而不需要精确估计action value或state value
- 大部分基于模型和不基于模型的算法都属于GPI

#### 算法步骤

每一episode中，

- 生成episode
- 策略评估与更新，从后面的步骤向前推（类似Prim算法一直加入小边进集合中得到最小生成树），以提高效率

#### Exploring Starts

- Exploring：指从每一个$(s,a)$出发，都要有这个episode，才能用后面生成的reward来计算return，从而推出该$(s,a)$的action value，要确保每一个$(s,a)$都被访问
- Starts：使得一定有episode的起始为$(s,a)$而不会被遗漏

## 3.MC ε-Greedy

#### soft policy

每一个行动都有可能去选择

**引入soft policy的原因**

- 一些足够长的epsiode能把所有(s,a)都包括在内，使得任意(s,a)都能被访问到，这样就可不用从每一个(s,a)出发来进行计算，即去除了exploring start这个条件

#### ε-Greedy policy

$$
\pi(a|s) = 
\left\{\begin{array}{left} 1 - \frac{ε}{|\mathcal{A}(s)|}(|\mathcal{A}(s)|-1), &for \ the \ greedy\ action
\\ \frac{ε}{|\mathcal{A}(s)|}, &for \ the \ other\ |\mathcal{A}(s)|-1\ actions
\end{array}\right.
$$

当$ε\in[0,1]$且$|\mathcal{A}(s)|$表示状态s所对应的动作的个数

选择贪心动作的概率始终比选择其他动作的概率要大

**选择 ε-greedy policy的原因**

- 平衡**利用exploitation**与**探索exploration**，使得做出的决策更好
  - 利用：已知一些状态能够带来更大的action value，应该在下一个时刻采用该action
  - 探索：由于当前知道的信息有限，有其他的action还未尝试过，可能未尝试过的action的value更大

#### 算法的改进

![image-20240516174110790](https://raw.githubusercontent.com/PasserByNaOH/PicGo/main/blogPic/image-20240516174110790.png)

使其不需要exploring starts的条件，但仍需要以不同的形式访问所有的(s,a)，对(s,a)采取every visit策略

**因此如果你想用 MC ε-Greedy的话，那么你的 ε 选择不能太大。或者用一个技巧，在最开始的时候 ε 比较大，探索性比较强，最后 ε 逐渐减小到 0，就可以得到一个最优的策略**
