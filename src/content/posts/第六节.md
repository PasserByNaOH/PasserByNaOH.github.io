---
title: 强化学习第六节
published: 2024-05-26
description: 强化学习相关概念
tags: [概念积累, 网络安全, 强化学习RL]
category: RL
draft: false
---
# 第六节——时序差分方法

## 1.TD（Time Distribution）算法

本质上是，**时序差分是从当前状态到下一个状态的奖励和下一个状态的估计值来更新当前状态的估计值。**

**TD算法是一个基于数据的算法**，以下形式的数据（data/experience）由策略$\pi$得出
$$
(s_0,r_1,s_1,...,s_t,r_{t+1},s_{t+1}) or\{(s_t,r_{t+1},s_{t+1})\}_t
$$

### 具体表示

TD算法的作用就是根据这些数据来估计这个策略$\pi$的状态值
$$
v_{t+1}(s_t)=v_t(s_t)-\alpha_t(s_t)[v_t(s_t)-[r_{t+1}+\gamma v_t(s_{t+1})]]① \\
v_{t+1}=v_t(s),\forall s \ne s_t,②
$$
①表示访问过后的状态值的变化

②表示没有被访问的状态，其状态值保持不变

![image-20240526155653247](https://raw.githubusercontent.com/PasserByNaOH/PicGo/main/blogPic/image-20240526155653247.png)

其中，

- $\bar{v}_t=r_{t+1}+\gamma v(s_{t+1})$被称为TD目标(TD target) 
  - 让$v(s_t)$朝着$\bar{v}_t$方向改进

- $\delta_t=v(s_t)-[r_{t+1}+\gamma v(s_{t+1})]=v(s_t)-\bar{v}_t$被称为TD误差(TD error)
  - 描述了在时间上的两步之间的不同之处
  - 反映了$v_t$和$v_{\pi}$之间的误差，用这个来改进当前估计

TD算法的作用是从一个给定的策略来估计当前状态的state value，而不是action value或得到最优策略



## 2.贝尔曼期望公式(TD算法的由来)

$$
v_{\pi}(s)=\mathbb{E}[R+\gamma v_{\pi}(S')|S = s],\ \ s\in S
$$

求解这个贝尔曼公式，即
$$
g(v(s))=v(s)-\mathbb{E}[R+\gamma v_{\pi}(S')|s]=0
$$
应用RM算法求解该问题
$$
v_{k+1}(s)=v_k(s)-\alpha_k\widetilde{g}(v_k(s)) \\
\widetilde{g}(v_k(s)) =v_k(s)-[r_k+\gamma v_{\pi}(s'_k)]
$$

- 由一个序列（trajectory）中的采样$\{(s_t,r_{t+1},s_{t+1})\}$来求解，如果访问了状态s就更新s的状态值，否则保持不变

- 由于不知道$v_{\pi}(s')$的真实值，则用该状态的估计值$v_k(s'_k)$来不断更新使其收敛，最后估计值就会收敛于$v_{\pi}(s')$

则得出**TD算法本质做的是针对一个给定的策略，做策略评估**（policy evaluation）

TD算法与蒙特卡洛算法的区别

![image-20240526180033005](https://raw.githubusercontent.com/PasserByNaOH/PicGo/main/blogPic/image-20240526180033005.png)

简而言之，**TD算法能够得到一个采样就估计一次，蒙特卡洛算法需要得到一个完整的episode再进行估计**

![image-20240526180340223](https://raw.githubusercontent.com/PasserByNaOH/PicGo/main/blogPic/image-20240526180340223.png)

简而言之，**TD算法能够根据过去的值对新的值进行预测，从而方差较小；蒙特卡洛算法只根据当前episode来估计，并不能根据其他episode来对新的episode进行估计**

## 3.Sarsa算法

Sarsa算法的目标是从给定一个策略来估计action value。

现在有数据$\{(s_t,a_t,r_{t+1},s_{t+1},a_{t+1})\}_t$，则由以下算法来更新action value
$$
q_{t+1}(s_t,a_t) = q_t(s_t,a_t)-\alpha_t(s_t,a_t)[q_t(s_t,a_t)-[r_{t+1}+\gamma q_t(s_{t+1},a_{t+1})]] \\
q_{t+1}(s,a) = q_t(s,a), \ \ \forall(s,a) \ne (s_t,a_t)
$$
其中，

- $q_t(s_t,a_t)$是$q_{\pi}(s_t,a_t)$在t时刻的估计值
- $\alpha_t(s_t,a_t)$是依赖于$s_t,a_t$的学习率 

其本质上和TD算法一样是解决一个给定策略的贝尔曼公式，不过Sarsa算法解决的贝尔曼公式来计算的是action value。

![image-20240527101249821](https://raw.githubusercontent.com/PasserByNaOH/PicGo/main/blogPic/image-20240527101249821.png)

Sarsa算法是从一个给定的初始状态，计算出到达目标状态的一条好策略（好路径），但不能计算出所有状态到达目标状态的最佳路径

## 3.Expected Sarsa算法

$$
q_{t+1}(s_t,a_t) = q_t(s_t,a_t)-\alpha_t(s_t,a_t)[q_t(s_t,a_t)-(r_{t+1}+\gamma \mathbb{E}[q_t(s_{t+1},A)])] \\
q_{t+1}(s,a) = q_t(s,a), \ \ \forall(s,a) \ne (s_t,a_t)
$$

其中，
$$
\mathbb{E}[q_t(s_{t+1},A)]=\sum_a \pi_t(a|s_{t+1})q_t(s_{t+1},a)=v_t(s_{t+1})
$$
**Expected Sarsa 使用期望值更新 Q 值**：

- 它计算下一状态所有可能的动作的期望 Q 值，而不是像 Sarsa 一样只使用实际选择的动作。
- 计算期望值需要遍历所有可能的动作，并使用策略的概率来加权每个动作的 Q 值。

Expected Sarsa算法本质上还是从求解一个贝尔曼公式得来,贝尔曼公式如下
$$
q_{\pi}(s,a)=\mathbb{E}[R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_t=s,A_t=a]
$$
由于和Sarsa算法一样是求解一个指定状态到达目标状态的最优路径，所以其他状态的最优路径不一定能够得到



缺点：动作越多，计算量越大

## 4.n-step Sarsa

结合了Sarsa算法和MC算法
$$
q_{t+1}(s_t,a_t) = q_t(s_t,a_t) - \alpha_t(s_t,a_t)[q_t(s_t,a_t)-[r_{t+1}+\gamma r_{t+2}+...+\gamma^nq_t(s_{t+n},a_{t+n})]]
$$
与其他Sarsa不同的就是TD target不同

- 当n=1时，该算法转变为Sarsa算法
- 当n=∞时，该算法转变为蒙特卡洛算法

所以，n-step Sarsa算法需要$(s_t,a_t,r_{t+1},s_{t+1},a_{t+1},...,r_{t+n},s_{t+n},a_{t+n})$，所以该算法需要过n个时刻再进行计算。

## 5.Q-Learning

### 定义

$$
q_{t+1}(s_t,a_t)=q_t(s_t,a_t)-\alpha_t(s_t,a_t)[q_t(s_t,a_t)-[r_{t+1}+\gamma \max_{a\in A}q_t(s_{t+1},a)]] \\
q_{t+1}(s,a)=q_t(s,a), \forall(s,a)\ne(s_t,a_t)
$$

该算法由以下贝尔曼最优方程得到
$$
q(s,a)=\mathbb{E}[R_{t+1}+\gamma \max_aq(S_{t+1},a)|S_t=s,A_t=a], \forall s,a
$$
![image-20240603113053826](https://raw.githubusercontent.com/PasserByNaOH/PicGo/main/blogPic/image-20240603113053826.png)

![image-20240616154435280](https://raw.githubusercontent.com/PasserByNaOH/PicGo/main/blogPic/image-20240616154435280.png)

### **有关策略的概念**

- behavior policy: 和环境进行交互然后生成数据样本（experience）
- target policy: 持续的更新从而得到最优的策略

以上两个是独立的概念，即一个策略即可两者都是，也可能是其中一个

- on-policy learning: behavior policy 和 target policy是相同的

  用这个策略和环境进行交互得到数据样本，并通过这些样本不断改进这个策略，再用新策略再和环境进行交互。（自己下棋变强）

- off-policy learning: behavior policy 和 target policy是不同的

  用一个策略和环境进行交互得到大量经验，用这些经验改进另一个策略，最后该第二策略收敛到最优的策略。（看别人下棋变强）

Sarsa、MC是on-policy、Q-learing是off-policy

**判断一个算法为on-policy或off-policy的方法**

- 判断该算法在数学上是解决什么问题
- 该算法运行需要的条件有哪些



# 本文代码

[5_TD(github.com)](https://github.com/PasserByNaOH/RL_Learing/tree/master/5_TD)
