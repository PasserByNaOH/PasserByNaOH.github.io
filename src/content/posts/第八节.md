---
title: 强化学习第七节
published: 2024-05-31
description: 强化学习相关概念
tags: [概念积累, 网络安全, 强化学习RL]
category: RL
draft: false
---
# 第八节——策略梯度下降

## 1.策略梯度的相关概念

### 策略的函数表示

$$
\pi(a|s,\theta)
$$

**函数代替表格方法的好处**：

当状态空间很大的时候，表格方法不能穷尽的表示所有状态，会造成计算效率低下。

**表格形式和函数形式的区别**：

- 定义最优策略的区别
  - 表格法：$\pi^*:V_{\pi}(S) \le V_{\pi^*}(S)$
  - 函数法：定义一个标量目标函数，再优化这个函数
- 判断一个动作发生的可能
  - 表格法：查表
  - 函数法：通过函数计算
- 更新策略的不同
  - 表格法：直接在表格里改
  - 函数法：通过更新函数中的参数$\theta$

**策略梯度定义**

- 取$J(\theta)$为目标函数

- 采用梯度下降来优化算法
  $$
  \theta_{t+1}=\theta_{t}+\alpha \nabla_{\theta}J(\theta_t)
  $$

## 2.状态平均值average state value

**参数方法与向量方法表示**
$$
\bar{v}_{\pi}=\sum _{s \in S}d(s)v_{\pi}(s)=d^Tv_{\pi}
$$

- $\bar{v}_{\pi}$是state value的加权平均

- $d(s) \ge 0$是权重

- $\sum_{s\in S}d(s) = 1$表示d(s)为状态s被选中的概率，此时目标函数可写为
  $$
  \bar{v}_{\pi}=\mathbb{E}[v_{\pi}(S)]
  $$

**概率分布d的选取**

1. d独立于策略$\pi$
   - 认为所有状态均匀分布
   - 可能特别关心某个状态
2. d依赖于策略$\pi$
   - 此时d为stationary distribution



## 3.平均单步奖励值average one-step reward

**第一种表现形式**：
$$
\begin{align}
\bar{r}_{\pi} &=\sum_{s \in S}d_{\pi}(s)r_{\pi}(s)=\mathbb{E}[r_{\pi}(S)] \\
r_{\pi}(s) &=\sum_{a \in A}\pi(a|s)r(s,a)

\end{align}
$$
其中，

- $r(s,a)$是当前的奖励值
- $d_{\pi}$是stationary distribution

**第二种表现形式**：

有一个策略，根据该策略形成了一个轨迹，并沿着该轨迹得到很多奖励$(R_{t+1},R_{t+2,.....})$，可以表示为
$$
\lim_{n\to \infty} \frac{1}{n}[\sum_{k=1}^{n}R_{t+k}|S=s_0]=\lim_{n\to \infty} \frac{1}{n}[\sum_{k=1}^{n}R_{t+k}]
$$
以上式子表示，从一个状态出发，走无穷多步的平均，走了无穷多步其实和初始状态没有什么关系了

## 4.目标函数的梯度

$$
\nabla_{\theta}J(\theta)=\sum_{s \in S}\eta(s)\sum_{a \in A}\nabla_{\theta}\pi(a|s,\theta)q_{\pi}(s,a)
$$

其中，

- $J(\theta)$可以是$\bar{v}_{\pi}、\bar{r}_{\pi}或\bar{v}_{\pi}^0$
- “=”不是严格的等号
- $\eta$在不同情况下会呈现出不同的分布

以上式子还可以写成以下，这个是真实的梯度
$$
\nabla_{\theta}J(\theta)=\mathbb{E}[\nabla \ln \pi(A|S,\theta)q_{\pi}(S,A)]
$$
其中，

- 由于对数函数的性质，$\pi(A|S,\theta)>0$
  - 为了实现以上性质，使用softmax函数将其归一化
  - 使用神经网络来实现
  - 该式是随机且具有探索性的



## 5.梯度上升算法

**定义**

- 梯度上升的算法做的事情是最大化$J(\theta)$
  $$
  \begin{align}
  \theta_{t+1} &=\theta_{t}+\alpha \nabla_{\theta}J(\theta) \\
  &=\theta_{t}+\alpha \mathbb{E}[\nabla_{\theta}\ln\pi(A|S,\theta)q_{\pi}(S,A)]
  \end{align}
  $$
  
- 由于以上是真实梯度，无法得到，用随机梯度代替，故有以下式子
  $$
  \theta_{t+1} =\theta_{t}+\alpha\nabla_{\theta}\ln\pi(a_t|s_t,\theta_t)q_{\pi}(s_t,a_t)
  $$

**对于采样问题**

- 对于S来说，其分布满足在策略$\pi$下的long run behavior
- 对于A来说
  - $A \sim \pi(A|S,\theta)$，故在$s_t$根据当前的$\pi(\theta_t)$采样$a_t$
  - 故该策略是on-policy算法

**理解该算法**

首先有
$$
\nabla_{\theta}\ln{\pi}(a_t|s_t,\theta_t) =\frac{\nabla_{\theta}\pi(a_t|s_t,\theta_t)}{\pi(a_t|s_t,\theta_t)}
$$
其次
$$
\begin{align}
\theta_{t+1} &=\theta_{t}+\alpha\nabla_{\theta}\ln\pi(a_t|s_t,\theta_t)q_{\pi}(s_t,a_t) \\
&= \theta_{t}+\alpha \underbrace{(\frac{q_t(s_t,a_t)}{\pi(a_t|s_t,\theta_t)})}_{\beta_t} \nabla_{\theta}\pi(a_t|s_t,\theta_t)
\end{align}
$$
最终，梯度上升算法表示为
$$
\theta_{t+1} =\theta_{t}+\alpha \beta_t\nabla_{\theta}\pi(a_t|s_t,\theta_t)
$$

- 当$\beta_t > 0$，$\pi(a_{t+1}|s_t,\theta_{t+1}) > \pi(a_t|s_t,\theta_t)$
- 当$\beta_t < 0$，$\pi(a_{t+1}|s_t,\theta_{t+1}) < \pi(a_t|s_t,\theta_t)$

故$\beta_t$可以很好的平衡算法的探索性和利用性

**REINFORCE算法**
$$
\theta_{t+1} =\theta_{t}+\alpha\nabla_{\theta}\ln\pi(a_t|s_t,\theta_t)q_{t}(s_t,a_t)
$$
即用$q_{t}(s_t,a_t)近似q_{\pi}(s_t,a_t)$

![image-20240531162037981](https://raw.githubusercontent.com/PasserByNaOH/PicGo/main/blogPic/image-20240531162037981.png)
