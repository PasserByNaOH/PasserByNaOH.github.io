---
title: 强化学习第一节
published: 2024-05-13
description: 强化学习相关概念
tags: [概念积累, 网络安全, 强化学习DL]
category: DL
draft: false
---
# 第一节——基本概念

## 1.State

即目标对于环境的一个状态，使用$s_i$来表示每一个状态，状态空间表示为$S=\{s_i\}^n_{i=1}$

## 2.Action

即目标在每个状态采取的行动，使用$a_i$表示每一个行动，行动空间表示为$\mathcal{A}(s_i)=\{a_i\}^n_{i=1}$

行动空间**依赖**于状态，即不同状态对应不同的行动空间

## 3.Reward

RL中的独有概念，表示目标在行动过后得到的值

- 如果Reward大于0，则表示行为受到鼓励
- 否则，表示不希望发生这样的行为
- 0表示什么都没发生

## 4.Trajectory 

轨迹，表示**状态-行动-奖励**链，如下
$$
s_1\xrightarrow[r=0]{a_2}s_2\xrightarrow[r=0]{a_3}s_5\xrightarrow[r=0]{a_3}s_8\xrightarrow[r=1]{a_2}s_9
$$


##  5.Return

返回值，等于整个轨迹上所有的奖励$r$加在一起的结果、

**discounted return**

返回值为无穷，引入discount rate = $γ\in[0,1)$ 
$$
discounted\ return = 0 + \gamma r_1+\gamma^2 r_2 + \gamma^3 r_3 + ……
$$
好处

- 使结果为有限值
- 平衡近远未来的返回值
  - 如果γ趋于0，则discounted return值由前面的reward决定
  - 反之趋于1，由后面的决定

## 6.Episode

- 从开始到停下来时候的状态的轨迹，称为一个Episode
- Episode通常都是有限步数



## 7.MDP

MDP代表“马尔可夫决策过程”（Markov Decision Process）。这是一个数学框架，用于描述一种随机的动态系统，在这种系统中，代理（通常是一个智能体或机器学习算法）采取行动，然后观察到环境的状态，从而获得奖励。

**马氏过程**

简单来说，**对于一个马式过程，当前状态之和上一个状态有关，和之前所有状态都无关**，**即计算当前状态发生的概率，只用上一个状态的取值来计算即可**。

如，考一个好高中，就可能考一个好大学，考一个好大学，就更有可能找一个好工作。

**马尔可夫决策过程**

在MDP中，添加了行为变量$A_t$与奖励变量$R_t$，则下一个时刻状态$S_t$由转移概率模型$P(S_{t+1}|S_t = s_t,A_t=a_t)$（马尔可夫可能性Markov property）决定，系统输出$R_t$由$P(R_t|S_t = s_t,A_t=a_t)$决定

如，在重点高中$S_{t}$努力学习$A_t$，考上好大学$R_t$的概率为90%，在重点高中不努力学习，考上好大学的概率为50%。

在普通中学努力学习，考上好大学的概率为60%，不努力概率为20%。

在MDP中，我们一般假定链的长度是有限的，并且一般不会太长。



# 课程来源

西湖大学——赵世钰

[B站课程链接](https://www.bilibili.com/video/BV1sd4y167NS/)
