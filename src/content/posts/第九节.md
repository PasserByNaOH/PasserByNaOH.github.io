---
title: 强化学习第七节
published: 2024-06-01
description: 强化学习相关概念
tags: [概念积累, 网络安全, 强化学习RL]
category: RL
draft: false
---
# 第九节——Actor-Critic

## 1.相关概念

- Actor:对应的是策略更新，用策略来指导采取行动
- Critic:对应的是策略评估

## 2.QAC（最简单的actor-critic）算法

![image-20240601093416374](https://raw.githubusercontent.com/PasserByNaOH/PicGo/main/blogPic/image-20240601093416374.png)

其中，

- 算法第一步得到的五个量$(s_t,a_t,r_{t+1},s_{t+1},a_{t+1})$，用sarsa算法(结合了值函数近似)估计action value，也就是下一步Critic
- 该算法为on-policy 
- QAC的Q代表的是Q值



## 3.A2C(Advantage actor-critic)算法

**最基本的思想**：

引入一个偏置量（baseline）来减少估计方差
$$
\begin{align}
\nabla_{\theta}J(\theta) &=\mathbb{E}_{S \sim \eta,A\sim \pi}[\nabla_{\theta}\ln\pi(A|S,\theta_t)q_{\pi}(S,A)] \\
&=\mathbb{E}_{S \sim \eta,A\sim \pi}[\nabla_{\theta}\ln\pi(A|S,\theta_t)q_{\pi}(S,A)-b(S)] \\
&=\mathbb{E}[X]（令上式为X）
\end{align}
$$

- 引入了新的b(S)对原式梯度没有影响
- 这个偏置值对X的方差有影响
- 故该算法的目标就是选择一个最好的偏置值来使方差var(X)最小，一般取$b(s)=\mathbb{E}_{A\sim\pi}[q(s,A)]=v_{\pi}(s)$

**advantage function**
$$
\delta_{\pi}(S,A)=q_{\pi}(S,A)-v_{\pi}(S)
$$
此时的梯度上升算法为
$$
\theta_{t+1} = \theta_{t}+\alpha \mathbb{E}[\nabla_{\theta}\ln\pi(A|S,\theta_t)\delta_{\pi}(S,A)]
$$
再用随机采样来去掉求期望
$$
\theta_{t+1} = \theta_{t}+\alpha\nabla_{\theta}\ln\pi(a_t|s_t,\theta_t)\delta_{t}(s_t,a_t)
$$
$\delta_{t}=q_t(s_t,a_t)-v_t(s_t)\to r_{t+1}+\gamma v_{t}(s_{t+1})-v_t(s_t)$即可替换为TD error

- 这样使得原本需要两个神经网络来近似$q_t和v_t$，现在只需要一个神经网络来近似$v_t$即可

故得到以下A2C算法

![image-20240601103644514](https://raw.githubusercontent.com/PasserByNaOH/PicGo/main/blogPic/image-20240601103644514.png)

## 4.Off-policy AC算法

首先有一个问题，已知一个概率分布p1及它的样本，但是想用这些样本求另外一个分布p0下的期望，要采用**重要性采样**。

### 重要性采样

$$
\mathbb{E}_{X\sim p_0}[X]=\sum_x p_0(x)x=\sum_x p_1(x) \underbrace{\frac{p_0(x)}{p_1(x)}}_{f(x)}x=\mathbb{E}_{X\sim p_1}[f(X)]
$$

故可以用p1来估计p0
$$
\mathbb{E}_{X\sim p_0}[X] \approx \bar{f}=\frac{1}{n}\sum_{i=1}^{n}f(x_i)=\frac{1}{n}\sum_{i=1}^{n}\frac{p_0(x_i)}{p_1(x_i)}x_i
$$
其中$\frac{p_0(x_i)}{p_1(x_i)}$称为importance weight

- 若$p_1(x_i)=p_0(x_i)$，此时$\bar{f}=\bar{x}$
- 若$p_0(x_i)\ge p_1(x_i)$，此时权重大于1，即给p1中的较少部分样本更大的权重

![image-20240601113731987](https://raw.githubusercontent.com/PasserByNaOH/PicGo/main/blogPic/image-20240601113731987.png)

### off-policy策略梯度相关理论

- 假设有一个behavior policy β，它被用来生成很多经验采样

- 目标是优化一个$J(\theta)$目标函数，$\theta$是target policy的参数
  $$
  J(\theta)=\sum_{s\in S}d_\beta(s)v_\pi(s)=\mathbb{E}_{S\sim d_\beta}[v_\pi(S)]
  $$
  

  其中$d_\beta$是在策略β对应的stattionary distribution

  ![](https://raw.githubusercontent.com/PasserByNaOH/PicGo/main/blogPic/image-20240601114531938.png)

  其中，$\frac{\pi(A|S,\theta)}{\beta(A|S)}$就是重要性采样

### Off-policy AC算法

![image-20240601114753534](https://raw.githubusercontent.com/PasserByNaOH/PicGo/main/blogPic/image-20240601114753534.png)

再用随机梯度代替求期望。得到算法

![image-20240601114834707](https://raw.githubusercontent.com/PasserByNaOH/PicGo/main/blogPic/image-20240601114834707.png)

![image-20240601115003134](https://raw.githubusercontent.com/PasserByNaOH/PicGo/main/blogPic/image-20240601115003134.png)

伪代码

![image-20240601115103161](https://raw.githubusercontent.com/PasserByNaOH/PicGo/main/blogPic/image-20240601115103161.png)

## 5.Deterministic actor-critic(DPG)

### 确定性策略梯度相关理论（deterministic policy gradient）

μ就是一个确定性的策略

![image-20240601121309531](https://raw.githubusercontent.com/PasserByNaOH/PicGo/main/blogPic/image-20240601121309531.png)

![image-20240601121509668](https://raw.githubusercontent.com/PasserByNaOH/PicGo/main/blogPic/image-20240601121509668.png)

伪代码

![image-20240601121532022](https://raw.githubusercontent.com/PasserByNaOH/PicGo/main/blogPic/image-20240601121532022.png)

- 该算法是off-policy，behavior policy是β，target policy为μ
- β 可以写成 μ+noise
- q(s,a,w)的选取
  - 线性方法
  - 神经网络方法



# 本文代码

[8_ActorCrtic (github.com)](https://github.com/PasserByNaOH/RL_Learing/tree/master/8_ActorCrtic)

[8_ActorCrtic (gitee.com)](https://gitee.com/PasserByNaOH/RL_Learing/tree/master/8_ActorCrtic)
